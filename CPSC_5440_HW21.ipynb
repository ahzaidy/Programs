{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMycRlzzNMKJ70v1xoeSj1G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahzaidy/Programs/blob/main/CPSC_5440_HW21.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGMaKqWQ7VBm",
        "outputId": "b15b19e6-6b39-495f-e726-43f3b8ad7519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Load CIFAR-100 dataset from Google Drive\n",
        "with open('/content/drive/My Drive/train', 'rb') as file:\n",
        "    train_dict = pickle.load(file, encoding='bytes')\n",
        "\n",
        "with open('/content/drive/My Drive/test', 'rb') as file:\n",
        "    test_dict = pickle.load(file, encoding='bytes')\n",
        "\n",
        "# Extract data and labels\n",
        "train_data = torch.tensor(train_dict[b'data'], dtype=torch.float32).reshape(-1, 3, 32, 32) / 255.0\n",
        "train_labels = torch.tensor(train_dict[b'fine_labels'], dtype=torch.long)\n",
        "test_data = torch.tensor(test_dict[b'data'], dtype=torch.float32).reshape(-1, 3, 32, 32) / 255.0\n",
        "test_labels = torch.tensor(test_dict[b'fine_labels'], dtype=torch.long)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = TensorDataset(train_data, train_labels)\n",
        "test_dataset = TensorDataset(test_data, test_labels)\n",
        "\n",
        "class CIFAR100Net(nn.Module):\n",
        "    def __init__(self, units, hidden_activation, output_activation):\n",
        "        super(CIFAR100Net, self).__init__()\n",
        "        self.hidden_activation = getattr(nn, hidden_activation)()\n",
        "        self.output_activation = getattr(nn, output_activation)()\n",
        "        self.fc1 = nn.Linear(3 * 32 * 32, units)\n",
        "        self.hidden_layers = nn.ModuleList([nn.Linear(units, units) for _ in range(4)])\n",
        "        self.output_layer = nn.Linear(units, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 3 * 32 * 32)\n",
        "        x = self.hidden_activation(self.fc1(x))\n",
        "        for layer in self.hidden_layers:\n",
        "            x = self.hidden_activation(layer(x))\n",
        "        x = self.output_activation(self.output_layer(x))\n",
        "        return x\n",
        "\n",
        "def train_cifar100(config, checkpoint_dir=None):\n",
        "    transform = transforms.Compose([transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "    train_size = int(0.8 * len(train_dataset))\n",
        "    val_size = len(train_dataset) - train_size\n",
        "    train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_subset, batch_size=config['batch_size'], shuffle=True)\n",
        "    val_loader = DataLoader(val_subset, batch_size=config['batch_size'], shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = CIFAR100Net(config['units'], config['hidden_activation'], config['output_activation']).to(device)\n",
        "\n",
        "    criterion = getattr(nn, config['loss'])()\n",
        "    optimizer = getattr(optim, config['optimizer'])(model.parameters(), lr=config['lr'])\n",
        "\n",
        "    for epoch in range(10):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy = correct / total\n",
        "        tune.report(loss=val_loss, accuracy=val_accuracy)\n",
        "\n",
        "config = {\n",
        "    'units': tune.choice([120, 240]),\n",
        "    'hidden_activation': tune.choice(['ReLU', 'Sigmoid']),\n",
        "    'output_activation': tune.choice(['Softmax', 'Sigmoid']),\n",
        "    'loss': tune.choice(['CrossEntropyLoss', 'MSELoss']),\n",
        "    'optimizer': tune.choice(['Adam', 'Adagrad']),\n",
        "    'batch_size': tune.choice([1000, 2000]),\n",
        "    'lr': tune.loguniform(1e-4, 1e-1)\n",
        "}\n",
        "\n",
        "ray.init(ignore_reinit_error=True)\n",
        "scheduler = ASHAScheduler(\n",
        "    metric='accuracy',\n",
        "    mode='max',\n",
        "    max_t=10,\n",
        "    grace_period=1,\n",
        "    reduction_factor=2\n",
        ")\n",
        "\n",
        "analysis = tune.run(\n",
        "    train_cifar100,\n",
        "    config=config,\n",
        "    num_samples=10,\n",
        "    scheduler=scheduler,\n",
        "    resources_per_trial={'cpu': 2, 'gpu': 1}\n",
        ")\n",
        "\n",
        "df = analysis.results_df\n",
        "best_trial = analysis.get_best_trial('accuracy', 'max', 'last')\n",
        "best_config = best_trial.config\n",
        "best_accuracy = best_trial.last_result['accuracy']\n",
        "\n",
        "print(f'Best Hyperparameters: {best_config}')\n",
        "print(f'Best Accuracy: {best_accuracy:.4f}')\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.lineplot(data=df, x='training_iteration', y='accuracy', hue='hidden_activation')\n",
        "plt.xlabel('Training Iteration')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.title('Hyperparameter Tuning Results')\n",
        "plt.grid()\n",
        "plt.savefig(\"/content/drive/My Drive/hyperparameter_tuning_plot.png\", dpi=300)\n",
        "plt.show()"
      ]
    }
  ]
}