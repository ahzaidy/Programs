{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMniJavA6wAqc0vX63TpOie",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahzaidy/Programs/blob/main/CPSC_5440_HW21.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xGMaKqWQ7VBm",
        "outputId": "4aa3501c-40cd-4d71-aba2-64675c2a0499"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-03-07 05:49:25,157\tINFO worker.py:1672 -- Calling ray.init() again after it has already been called.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------+\n",
            "| Configuration for experiment     train_cifar100_2025-03-07_05-49-27   |\n",
            "+-----------------------------------------------------------------------+\n",
            "| Search algorithm                 BasicVariantGenerator                |\n",
            "| Scheduler                        AsyncHyperBandScheduler              |\n",
            "| Number of trials                 10                                   |\n",
            "+-----------------------------------------------------------------------+\n",
            "\n",
            "View detailed results here: /root/ray_results/train_cifar100_2025-03-07_05-49-27\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2025-03-07_05-45-45_878621_2122/artifacts/2025-03-07_05-49-27/train_cifar100_2025-03-07_05-49-27/driver_artifacts`\n",
            "\n",
            "Trial status: 10 PENDING\n",
            "Current time: 2025-03-07 05:49:28. Total running time: 0s\n",
            "Logical resource usage: 0/2 CPUs, 0/0 GPUs\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                   status       units   hidden_activation     output_activation     loss               optimizer       batch_size            lr |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar100_ee81f_00000   PENDING        120   Sigmoid               Softmax               MSELoss            Adam                  1000   0.00433892  |\n",
            "| train_cifar100_ee81f_00001   PENDING        120   Sigmoid               Sigmoid               MSELoss            Adam                  1000   0.000139288 |\n",
            "| train_cifar100_ee81f_00002   PENDING        120   ReLU                  Sigmoid               MSELoss            Adagrad               2000   0.000721523 |\n",
            "| train_cifar100_ee81f_00003   PENDING        120   ReLU                  Softmax               CrossEntropyLoss   Adagrad               1000   0.000213033 |\n",
            "| train_cifar100_ee81f_00004   PENDING        120   ReLU                  Sigmoid               MSELoss            Adam                  1000   0.0162642   |\n",
            "| train_cifar100_ee81f_00005   PENDING        120   ReLU                  Softmax               MSELoss            Adam                  1000   0.00106151  |\n",
            "| train_cifar100_ee81f_00006   PENDING        120   ReLU                  Softmax               MSELoss            Adagrad               1000   0.000534782 |\n",
            "| train_cifar100_ee81f_00007   PENDING        240   Sigmoid               Sigmoid               MSELoss            Adam                  2000   0.00662757  |\n",
            "| train_cifar100_ee81f_00008   PENDING        240   Sigmoid               Softmax               CrossEntropyLoss   Adam                  2000   0.000852801 |\n",
            "| train_cifar100_ee81f_00009   PENDING        240   Sigmoid               Sigmoid               CrossEntropyLoss   Adagrad               2000   0.00145283  |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(autoscaler +3m51s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
            "\u001b[33m(autoscaler +3m51s)\u001b[0m Error: No available node types can fulfill resource request {'CPU': 2.0, 'GPU': 1.0}. Add suitable node types to this cluster to resolve this issue.\n",
            "Trial status: 10 PENDING\n",
            "Current time: 2025-03-07 05:49:58. Total running time: 30s\n",
            "Logical resource usage: 0/2 CPUs, 0/0 GPUs\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                   status       units   hidden_activation     output_activation     loss               optimizer       batch_size            lr |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar100_ee81f_00000   PENDING        120   Sigmoid               Softmax               MSELoss            Adam                  1000   0.00433892  |\n",
            "| train_cifar100_ee81f_00001   PENDING        120   Sigmoid               Sigmoid               MSELoss            Adam                  1000   0.000139288 |\n",
            "| train_cifar100_ee81f_00002   PENDING        120   ReLU                  Sigmoid               MSELoss            Adagrad               2000   0.000721523 |\n",
            "| train_cifar100_ee81f_00003   PENDING        120   ReLU                  Softmax               CrossEntropyLoss   Adagrad               1000   0.000213033 |\n",
            "| train_cifar100_ee81f_00004   PENDING        120   ReLU                  Sigmoid               MSELoss            Adam                  1000   0.0162642   |\n",
            "| train_cifar100_ee81f_00005   PENDING        120   ReLU                  Softmax               MSELoss            Adam                  1000   0.00106151  |\n",
            "| train_cifar100_ee81f_00006   PENDING        120   ReLU                  Softmax               MSELoss            Adagrad               1000   0.000534782 |\n",
            "| train_cifar100_ee81f_00007   PENDING        240   Sigmoid               Sigmoid               MSELoss            Adam                  2000   0.00662757  |\n",
            "| train_cifar100_ee81f_00008   PENDING        240   Sigmoid               Softmax               CrossEntropyLoss   Adam                  2000   0.000852801 |\n",
            "| train_cifar100_ee81f_00009   PENDING        240   Sigmoid               Sigmoid               CrossEntropyLoss   Adagrad               2000   0.00145283  |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[33m(autoscaler +4m26s)\u001b[0m Error: No available node types can fulfill resource request {'CPU': 2.0, 'GPU': 1.0}. Add suitable node types to this cluster to resolve this issue.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-03-07 05:50:28,307\tWARNING insufficient_resources_manager.py:163 -- Ignore this message if the cluster is autoscaling. No trial is running and no new trial has been started within the last 60 seconds. This could be due to the cluster not having enough resources available. You asked for 2.0 CPUs and 1.0 GPUs per trial, but the cluster only has 2.0 CPUs and 0 GPUs available. Stop the tuning and adjust the required resources (e.g. via the `ScalingConfig` or `resources_per_trial`, or `num_workers` for rllib), or add more resources to your cluster.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial status: 10 PENDING\n",
            "Current time: 2025-03-07 05:50:28. Total running time: 1min 0s\n",
            "Logical resource usage: 0/2 CPUs, 0/0 GPUs\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                   status       units   hidden_activation     output_activation     loss               optimizer       batch_size            lr |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar100_ee81f_00000   PENDING        120   Sigmoid               Softmax               MSELoss            Adam                  1000   0.00433892  |\n",
            "| train_cifar100_ee81f_00001   PENDING        120   Sigmoid               Sigmoid               MSELoss            Adam                  1000   0.000139288 |\n",
            "| train_cifar100_ee81f_00002   PENDING        120   ReLU                  Sigmoid               MSELoss            Adagrad               2000   0.000721523 |\n",
            "| train_cifar100_ee81f_00003   PENDING        120   ReLU                  Softmax               CrossEntropyLoss   Adagrad               1000   0.000213033 |\n",
            "| train_cifar100_ee81f_00004   PENDING        120   ReLU                  Sigmoid               MSELoss            Adam                  1000   0.0162642   |\n",
            "| train_cifar100_ee81f_00005   PENDING        120   ReLU                  Softmax               MSELoss            Adam                  1000   0.00106151  |\n",
            "| train_cifar100_ee81f_00006   PENDING        120   ReLU                  Softmax               MSELoss            Adagrad               1000   0.000534782 |\n",
            "| train_cifar100_ee81f_00007   PENDING        240   Sigmoid               Sigmoid               MSELoss            Adam                  2000   0.00662757  |\n",
            "| train_cifar100_ee81f_00008   PENDING        240   Sigmoid               Softmax               CrossEntropyLoss   Adam                  2000   0.000852801 |\n",
            "| train_cifar100_ee81f_00009   PENDING        240   Sigmoid               Sigmoid               CrossEntropyLoss   Adagrad               2000   0.00145283  |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[33m(autoscaler +5m1s)\u001b[0m Error: No available node types can fulfill resource request {'CPU': 2.0, 'GPU': 1.0}. Add suitable node types to this cluster to resolve this issue.\n",
            "Trial status: 10 PENDING\n",
            "Current time: 2025-03-07 05:50:58. Total running time: 1min 30s\n",
            "Logical resource usage: 0/2 CPUs, 0/0 GPUs\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                   status       units   hidden_activation     output_activation     loss               optimizer       batch_size            lr |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar100_ee81f_00000   PENDING        120   Sigmoid               Softmax               MSELoss            Adam                  1000   0.00433892  |\n",
            "| train_cifar100_ee81f_00001   PENDING        120   Sigmoid               Sigmoid               MSELoss            Adam                  1000   0.000139288 |\n",
            "| train_cifar100_ee81f_00002   PENDING        120   ReLU                  Sigmoid               MSELoss            Adagrad               2000   0.000721523 |\n",
            "| train_cifar100_ee81f_00003   PENDING        120   ReLU                  Softmax               CrossEntropyLoss   Adagrad               1000   0.000213033 |\n",
            "| train_cifar100_ee81f_00004   PENDING        120   ReLU                  Sigmoid               MSELoss            Adam                  1000   0.0162642   |\n",
            "| train_cifar100_ee81f_00005   PENDING        120   ReLU                  Softmax               MSELoss            Adam                  1000   0.00106151  |\n",
            "| train_cifar100_ee81f_00006   PENDING        120   ReLU                  Softmax               MSELoss            Adagrad               1000   0.000534782 |\n",
            "| train_cifar100_ee81f_00007   PENDING        240   Sigmoid               Sigmoid               MSELoss            Adam                  2000   0.00662757  |\n",
            "| train_cifar100_ee81f_00008   PENDING        240   Sigmoid               Softmax               CrossEntropyLoss   Adam                  2000   0.000852801 |\n",
            "| train_cifar100_ee81f_00009   PENDING        240   Sigmoid               Sigmoid               CrossEntropyLoss   Adagrad               2000   0.00145283  |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[33m(autoscaler +5m37s)\u001b[0m Error: No available node types can fulfill resource request {'CPU': 2.0, 'GPU': 1.0}. Add suitable node types to this cluster to resolve this issue.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-03-07 05:51:28,360\tWARNING insufficient_resources_manager.py:163 -- Ignore this message if the cluster is autoscaling. No trial is running and no new trial has been started within the last 60 seconds. This could be due to the cluster not having enough resources available. You asked for 2.0 CPUs and 1.0 GPUs per trial, but the cluster only has 2.0 CPUs and 0 GPUs available. Stop the tuning and adjust the required resources (e.g. via the `ScalingConfig` or `resources_per_trial`, or `num_workers` for rllib), or add more resources to your cluster.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial status: 10 PENDING\n",
            "Current time: 2025-03-07 05:51:28. Total running time: 2min 0s\n",
            "Logical resource usage: 0/2 CPUs, 0/0 GPUs\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                   status       units   hidden_activation     output_activation     loss               optimizer       batch_size            lr |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar100_ee81f_00000   PENDING        120   Sigmoid               Softmax               MSELoss            Adam                  1000   0.00433892  |\n",
            "| train_cifar100_ee81f_00001   PENDING        120   Sigmoid               Sigmoid               MSELoss            Adam                  1000   0.000139288 |\n",
            "| train_cifar100_ee81f_00002   PENDING        120   ReLU                  Sigmoid               MSELoss            Adagrad               2000   0.000721523 |\n",
            "| train_cifar100_ee81f_00003   PENDING        120   ReLU                  Softmax               CrossEntropyLoss   Adagrad               1000   0.000213033 |\n",
            "| train_cifar100_ee81f_00004   PENDING        120   ReLU                  Sigmoid               MSELoss            Adam                  1000   0.0162642   |\n",
            "| train_cifar100_ee81f_00005   PENDING        120   ReLU                  Softmax               MSELoss            Adam                  1000   0.00106151  |\n",
            "| train_cifar100_ee81f_00006   PENDING        120   ReLU                  Softmax               MSELoss            Adagrad               1000   0.000534782 |\n",
            "| train_cifar100_ee81f_00007   PENDING        240   Sigmoid               Sigmoid               MSELoss            Adam                  2000   0.00662757  |\n",
            "| train_cifar100_ee81f_00008   PENDING        240   Sigmoid               Softmax               CrossEntropyLoss   Adam                  2000   0.000852801 |\n",
            "| train_cifar100_ee81f_00009   PENDING        240   Sigmoid               Sigmoid               CrossEntropyLoss   Adagrad               2000   0.00145283  |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-03-07 05:51:29,371\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
            "2025-03-07 05:51:29,376\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/train_cifar100_2025-03-07_05-49-27' in 0.0039s.\n",
            "2025-03-07 05:51:29,389\tWARNING tune.py:1056 -- Experiment has been interrupted, but the most recent state was saved.\n",
            "Resume experiment with: tune.run(..., resume=True)\n",
            "2025-03-07 05:51:29,400\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 10 trial(s):\n",
            "- train_cifar100_ee81f_00000: FileNotFoundError('Could not fetch metrics for train_cifar100_ee81f_00000: both result.json and progress.csv were not found at /root/ray_results/train_cifar100_2025-03-07_05-49-27/train_cifar100_ee81f_00000_0_batch_size=1000,hidden_activation=Sigmoid,loss=MSELoss,lr=0.0043,optimizer=Adam,output_activation=Sof_2025-03-07_05-49-27')\n",
            "- train_cifar100_ee81f_00001: FileNotFoundError('Could not fetch metrics for train_cifar100_ee81f_00001: both result.json and progress.csv were not found at /root/ray_results/train_cifar100_2025-03-07_05-49-27/train_cifar100_ee81f_00001_1_batch_size=1000,hidden_activation=Sigmoid,loss=MSELoss,lr=0.0001,optimizer=Adam,output_activation=Sig_2025-03-07_05-49-27')\n",
            "- train_cifar100_ee81f_00002: FileNotFoundError('Could not fetch metrics for train_cifar100_ee81f_00002: both result.json and progress.csv were not found at /root/ray_results/train_cifar100_2025-03-07_05-49-27/train_cifar100_ee81f_00002_2_batch_size=2000,hidden_activation=ReLU,loss=MSELoss,lr=0.0007,optimizer=Adagrad,output_activation=Sig_2025-03-07_05-49-27')\n",
            "- train_cifar100_ee81f_00003: FileNotFoundError('Could not fetch metrics for train_cifar100_ee81f_00003: both result.json and progress.csv were not found at /root/ray_results/train_cifar100_2025-03-07_05-49-27/train_cifar100_ee81f_00003_3_batch_size=1000,hidden_activation=ReLU,loss=CrossEntropyLoss,lr=0.0002,optimizer=Adagrad,output_activ_2025-03-07_05-49-28')\n",
            "- train_cifar100_ee81f_00004: FileNotFoundError('Could not fetch metrics for train_cifar100_ee81f_00004: both result.json and progress.csv were not found at /root/ray_results/train_cifar100_2025-03-07_05-49-27/train_cifar100_ee81f_00004_4_batch_size=1000,hidden_activation=ReLU,loss=MSELoss,lr=0.0163,optimizer=Adam,output_activation=Sigmoi_2025-03-07_05-49-28')\n",
            "- train_cifar100_ee81f_00005: FileNotFoundError('Could not fetch metrics for train_cifar100_ee81f_00005: both result.json and progress.csv were not found at /root/ray_results/train_cifar100_2025-03-07_05-49-27/train_cifar100_ee81f_00005_5_batch_size=1000,hidden_activation=ReLU,loss=MSELoss,lr=0.0011,optimizer=Adam,output_activation=Softma_2025-03-07_05-49-28')\n",
            "- train_cifar100_ee81f_00006: FileNotFoundError('Could not fetch metrics for train_cifar100_ee81f_00006: both result.json and progress.csv were not found at /root/ray_results/train_cifar100_2025-03-07_05-49-27/train_cifar100_ee81f_00006_6_batch_size=1000,hidden_activation=ReLU,loss=MSELoss,lr=0.0005,optimizer=Adagrad,output_activation=Sof_2025-03-07_05-49-28')\n",
            "- train_cifar100_ee81f_00007: FileNotFoundError('Could not fetch metrics for train_cifar100_ee81f_00007: both result.json and progress.csv were not found at /root/ray_results/train_cifar100_2025-03-07_05-49-27/train_cifar100_ee81f_00007_7_batch_size=2000,hidden_activation=Sigmoid,loss=MSELoss,lr=0.0066,optimizer=Adam,output_activation=Sig_2025-03-07_05-49-28')\n",
            "- train_cifar100_ee81f_00008: FileNotFoundError('Could not fetch metrics for train_cifar100_ee81f_00008: both result.json and progress.csv were not found at /root/ray_results/train_cifar100_2025-03-07_05-49-27/train_cifar100_ee81f_00008_8_batch_size=2000,hidden_activation=Sigmoid,loss=CrossEntropyLoss,lr=0.0009,optimizer=Adam,output_activ_2025-03-07_05-49-28')\n",
            "- train_cifar100_ee81f_00009: FileNotFoundError('Could not fetch metrics for train_cifar100_ee81f_00009: both result.json and progress.csv were not found at /root/ray_results/train_cifar100_2025-03-07_05-49-27/train_cifar100_ee81f_00009_9_batch_size=2000,hidden_activation=Sigmoid,loss=CrossEntropyLoss,lr=0.0015,optimizer=Adagrad,output_ac_2025-03-07_05-49-28')\n",
            "2025-03-07 05:51:29,414\tWARNING experiment_analysis.py:558 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial status: 10 PENDING\n",
            "Current time: 2025-03-07 05:51:29. Total running time: 2min 1s\n",
            "Logical resource usage: 0/2 CPUs, 0/0 GPUs\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                   status       units   hidden_activation     output_activation     loss               optimizer       batch_size            lr |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar100_ee81f_00000   PENDING        120   Sigmoid               Softmax               MSELoss            Adam                  1000   0.00433892  |\n",
            "| train_cifar100_ee81f_00001   PENDING        120   Sigmoid               Sigmoid               MSELoss            Adam                  1000   0.000139288 |\n",
            "| train_cifar100_ee81f_00002   PENDING        120   ReLU                  Sigmoid               MSELoss            Adagrad               2000   0.000721523 |\n",
            "| train_cifar100_ee81f_00003   PENDING        120   ReLU                  Softmax               CrossEntropyLoss   Adagrad               1000   0.000213033 |\n",
            "| train_cifar100_ee81f_00004   PENDING        120   ReLU                  Sigmoid               MSELoss            Adam                  1000   0.0162642   |\n",
            "| train_cifar100_ee81f_00005   PENDING        120   ReLU                  Softmax               MSELoss            Adam                  1000   0.00106151  |\n",
            "| train_cifar100_ee81f_00006   PENDING        120   ReLU                  Softmax               MSELoss            Adagrad               1000   0.000534782 |\n",
            "| train_cifar100_ee81f_00007   PENDING        240   Sigmoid               Sigmoid               MSELoss            Adam                  2000   0.00662757  |\n",
            "| train_cifar100_ee81f_00008   PENDING        240   Sigmoid               Softmax               CrossEntropyLoss   Adam                  2000   0.000852801 |\n",
            "| train_cifar100_ee81f_00009   PENDING        240   Sigmoid               Sigmoid               CrossEntropyLoss   Adagrad               2000   0.00145283  |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'config'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e9740d36b5c6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0mbest_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'max'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'last'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m \u001b[0mbest_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0mbest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'config'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Load CIFAR-100 dataset from Google Drive\n",
        "with open('/content/drive/My Drive/train', 'rb') as file:\n",
        "    train_dict = pickle.load(file, encoding='bytes')\n",
        "\n",
        "with open('/content/drive/My Drive/test', 'rb') as file:\n",
        "    test_dict = pickle.load(file, encoding='bytes')\n",
        "\n",
        "# Extract data and labels\n",
        "train_data = torch.tensor(train_dict[b'data'], dtype=torch.float32).reshape(-1, 3, 32, 32) / 255.0\n",
        "train_labels = torch.tensor(train_dict[b'fine_labels'], dtype=torch.long)\n",
        "test_data = torch.tensor(test_dict[b'data'], dtype=torch.float32).reshape(-1, 3, 32, 32) / 255.0\n",
        "test_labels = torch.tensor(test_dict[b'fine_labels'], dtype=torch.long)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = TensorDataset(train_data, train_labels)\n",
        "test_dataset = TensorDataset(test_data, test_labels)\n",
        "\n",
        "class CIFAR100Net(nn.Module):\n",
        "    def __init__(self, units, hidden_activation, output_activation):\n",
        "        super(CIFAR100Net, self).__init__()\n",
        "        self.hidden_activation = getattr(nn, hidden_activation)()\n",
        "        self.output_activation = getattr(nn, output_activation)()\n",
        "        self.fc1 = nn.Linear(3 * 32 * 32, units)\n",
        "        self.hidden_layers = nn.ModuleList([nn.Linear(units, units) for _ in range(4)])\n",
        "        self.output_layer = nn.Linear(units, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 3 * 32 * 32)\n",
        "        x = self.hidden_activation(self.fc1(x))\n",
        "        for layer in self.hidden_layers:\n",
        "            x = self.hidden_activation(layer(x))\n",
        "        x = self.output_activation(self.output_layer(x))\n",
        "        return x\n",
        "\n",
        "def train_cifar100(config, train_dataset=None, test_dataset=None, checkpoint_dir=None):\n",
        "    transform = transforms.Compose([transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "    train_size = int(0.8 * len(train_dataset))\n",
        "    val_size = len(train_dataset) - train_size\n",
        "    train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_subset, batch_size=config['batch_size'], shuffle=True)\n",
        "    val_loader = DataLoader(val_subset, batch_size=config['batch_size'], shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "    device = torch.device(\"cpu\")\n",
        "    model = CIFAR100Net(config['units'], config['hidden_activation'], config['output_activation']).to(device)\n",
        "\n",
        "    criterion = getattr(nn, config['loss'])()\n",
        "    optimizer = getattr(optim, config['optimizer'])(model.parameters(), lr=config['lr'])\n",
        "\n",
        "    for epoch in range(10):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy = correct / total\n",
        "        tune.report(loss=val_loss, accuracy=val_accuracy)\n",
        "\n",
        "config = {\n",
        "    'units': tune.choice([120, 240]),\n",
        "    'hidden_activation': tune.choice(['ReLU', 'Sigmoid']),\n",
        "    'output_activation': tune.choice(['Softmax', 'Sigmoid']),\n",
        "    'loss': tune.choice(['CrossEntropyLoss', 'MSELoss']),\n",
        "    'optimizer': tune.choice(['Adam', 'Adagrad']),\n",
        "    'batch_size': tune.choice([1000, 2000]),\n",
        "    'lr': tune.loguniform(1e-4, 1e-1)\n",
        "}\n",
        "\n",
        "ray.init(ignore_reinit_error=True)\n",
        "scheduler = ASHAScheduler(\n",
        "    metric='accuracy',\n",
        "    mode='max',\n",
        "    max_t=10,\n",
        "    grace_period=1,\n",
        "    reduction_factor=2\n",
        ")\n",
        "\n",
        "analysis = tune.run(\n",
        "    tune.with_parameters(train_cifar100, train_dataset=train_dataset, test_dataset=test_dataset),\n",
        "    config=config,\n",
        "    num_samples=10,\n",
        "    scheduler=scheduler,\n",
        "    resources_per_trial={'cpu': 2}\n",
        ")\n",
        "\n",
        "df = analysis.results_df\n",
        "best_trial = analysis.get_best_trial('accuracy', 'max', 'last')\n",
        "best_config = best_trial.config\n",
        "best_accuracy = best_trial.last_result['accuracy']\n",
        "\n",
        "print(f'Best Hyperparameters: {best_config}')\n",
        "print(f'Best Accuracy: {best_accuracy:.4f}')\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.lineplot(data=df, x='training_iteration', y='accuracy', hue='hidden_activation')\n",
        "plt.xlabel('Training Iteration')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.title('Hyperparameter Tuning Results')\n",
        "plt.grid()\n",
        "plt.savefig(\"/content/drive/My Drive/hyperparameter_tuning_plot.png\", dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ray[tune]"
      ],
      "metadata": {
        "id": "lK9riAx4IY2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ray"
      ],
      "metadata": {
        "id": "Mn0undXuH4La"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}