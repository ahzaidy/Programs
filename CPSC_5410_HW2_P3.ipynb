{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTXsxhf4vrIOtbFZxU6xLM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahzaidy/Programs/blob/main/CPSC_5410_HW2_P3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUcro8FErxVK"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "#Author: Arif H. Zaidy                                                        #\n",
        "#Date: March 10, 2025                                                         #\n",
        "#Course: CPSC 5410                                                            #\n",
        "#Topic: Assignment 2, Question 3                                              #\n",
        "#Description:                                                                 #\n",
        "#This program performs dimensionality reduction on a subset of the MNIST      #\n",
        "#dataset containing digits 0, 1, and 2. It applies both Linear Discriminant   #\n",
        "#Analysis (LDA) and Principal Component Analysis (PCA) to visualize the       #\n",
        "#data in a reduced 2D space. The dataset is split into training (80%) and     #\n",
        "#testing (20%) while maintaining class balance. Scatter plots are generated to#\n",
        "#compare LDA and PCA projections. Additionally, the program explains why LDA  #\n",
        "#cannot project data into 3D space when dealing with three classes.           #\n",
        "###############################################################################\n",
        "\n",
        "# Including Python libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load dataset\n",
        "# The dataset contains MNIST digits 0, 1, and 2.\n",
        "df = pd.read_csv(\"MNIST digits0-1-2.csv\")\n",
        "X = df.iloc[:, :-1].values  # Extract feature matrix (pixel values)\n",
        "y = df.iloc[:, -1].values   # Extract labels (digit classes)\n",
        "\n",
        "# Split data into training (80%) and testing (20%) while maintaining class balance\n",
        "train_indices, test_indices = [], []\n",
        "for label in np.unique(y):  # Iterate through each unique class (0,1,2)\n",
        "    indices = np.where(y == label)[0]  # Get indices of samples belonging to the class\n",
        "    train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)  # Split class data\n",
        "    train_indices.extend(train_idx)\n",
        "    test_indices.extend(test_idx)\n",
        "\n",
        "# Create training and testing datasets\n",
        "X_train, X_test = X[train_indices], X[test_indices]\n",
        "y_train, y_test = y[train_indices], y[test_indices]\n",
        "\n",
        "# Apply Linear Discriminant Analysis (LDA) for dimensionality reduction\n",
        "lda = LinearDiscriminantAnalysis(n_components=2)  # Reduce to 2D space\n",
        "X_train_lda = lda.fit_transform(X_train, y_train)  # Fit LDA and transform training data\n",
        "X_test_lda = lda.transform(X_test)  # Transform test data using the trained LDA model\n",
        "\n",
        "# Scatter plot to visualize LDA-projected data\n",
        "plt.figure(figsize=(10, 5))\n",
        "for label, marker, color in zip([0, 1, 2], ['o', 's', 'D'], ['r', 'g', 'b']):\n",
        "    # Plot training data points\n",
        "    plt.scatter(X_train_lda[y_train == label, 0], X_train_lda[y_train == label, 1],\n",
        "                marker=marker, color=color, alpha=0.6, label=f'Train {label}')\n",
        "    # Plot testing data points\n",
        "    plt.scatter(X_test_lda[y_test == label, 0], X_test_lda[y_test == label, 1],\n",
        "                marker=marker, color=color, edgecolors='k', label=f'Test {label}')\n",
        "\n",
        "plt.title(\"LDA Projection of MNIST Digits (0,1,2)\")\n",
        "plt.xlabel(\"LD1\")  # First linear discriminant axis\n",
        "plt.ylabel(\"LD2\")  # Second linear discriminant axis\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Apply Principal Component Analysis (PCA) for comparison\n",
        "pca = PCA(n_components=2)  # Reduce to 2D space\n",
        "X_train_pca = pca.fit_transform(X_train)  # Fit PCA and transform training data\n",
        "X_test_pca = pca.transform(X_test)  # Transform test data using the trained PCA model\n",
        "\n",
        "# Scatter plot to visualize PCA-projected data\n",
        "plt.figure(figsize=(10, 5))\n",
        "for label, marker, color in zip([0, 1, 2], ['o', 's', 'D'], ['r', 'g', 'b']):\n",
        "    # Plot training data points\n",
        "    plt.scatter(X_train_pca[y_train == label, 0], X_train_pca[y_train == label, 1],\n",
        "                marker=marker, color=color, alpha=0.6, label=f'Train {label}')\n",
        "    # Plot testing data points\n",
        "    plt.scatter(X_test_pca[y_test == label, 0], X_test_pca[y_test == label, 1],\n",
        "                marker=marker, color=color, edgecolors='k', label=f'Test {label}')\n",
        "\n",
        "plt.title(\"PCA Projection of MNIST Digits (0,1,2)\")\n",
        "plt.xlabel(\"PC1\")  # First principal component\n",
        "plt.ylabel(\"PC2\")  # Second principal component\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Explanation of LDA's dimensionality constraint\n",
        "print(\"LDA can project data into at most (c-1) dimensions, where c is the number of classes.\")\n",
        "print(\"Since we have 3 classes (0,1,2), the maximum dimension for LDA is 2 (3-1). Thus, LDA cannot project to 3D space.\")"
      ]
    }
  ]
}