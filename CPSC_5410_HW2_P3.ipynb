{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtHQIhFdb7/Hu/YRN9xF3W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahzaidy/Programs/blob/main/CPSC_5410_HW2_P3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUcro8FErxVK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"MNIST digits0-1-2.csv\")\n",
        "X = df.iloc[:, :-1].values  # Features\n",
        "y = df.iloc[:, -1].values   # Labels\n",
        "\n",
        "# Split data (80% training, 20% testing for each class)\n",
        "train_indices, test_indices = [], []\n",
        "for label in np.unique(y):\n",
        "    indices = np.where(y == label)[0]\n",
        "    train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
        "    train_indices.extend(train_idx)\n",
        "    test_indices.extend(test_idx)\n",
        "\n",
        "X_train, X_test = X[train_indices], X[test_indices]\n",
        "y_train, y_test = y[train_indices], y[test_indices]\n",
        "\n",
        "# Apply LDA with 2 components\n",
        "lda = LinearDiscriminantAnalysis(n_components=2)\n",
        "X_train_lda = lda.fit_transform(X_train, y_train)\n",
        "X_test_lda = lda.transform(X_test)\n",
        "\n",
        "# Scatter plot for LDA projection\n",
        "plt.figure(figsize=(10, 5))\n",
        "for label, marker, color in zip([0, 1, 2], ['o', 's', 'D'], ['r', 'g', 'b']):\n",
        "    plt.scatter(X_train_lda[y_train == label, 0], X_train_lda[y_train == label, 1],\n",
        "                marker=marker, color=color, alpha=0.6, label=f'Train {label}')\n",
        "    plt.scatter(X_test_lda[y_test == label, 0], X_test_lda[y_test == label, 1],\n",
        "                marker=marker, color=color, edgecolors='k', label=f'Test {label}')\n",
        "\n",
        "plt.title(\"LDA Projection of MNIST Digits (0,1,2)\")\n",
        "plt.xlabel(\"LD1\")\n",
        "plt.ylabel(\"LD2\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Compare with PCA projection\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "for label, marker, color in zip([0, 1, 2], ['o', 's', 'D'], ['r', 'g', 'b']):\n",
        "    plt.scatter(X_train_pca[y_train == label, 0], X_train_pca[y_train == label, 1],\n",
        "                marker=marker, color=color, alpha=0.6, label=f'Train {label}')\n",
        "    plt.scatter(X_test_pca[y_test == label, 0], X_test_pca[y_test == label, 1],\n",
        "                marker=marker, color=color, edgecolors='k', label=f'Test {label}')\n",
        "\n",
        "plt.title(\"PCA Projection of MNIST Digits (0,1,2)\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Explanation of 3D LDA projection\n",
        "print(\"LDA can project data into at most (c-1) dimensions, where c is the number of classes.\")\n",
        "print(\"Since we have 3 classes (0,1,2), the maximum dimension for LDA is 2 (3-1). Thus, LDA cannot project to 3D space.\")"
      ]
    }
  ]
}