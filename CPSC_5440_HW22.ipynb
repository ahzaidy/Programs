{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahzaidy/Programs/blob/main/CPSC_5440_HW22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iuZVNMWq96F"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "#Author: Arif H. Zaidy                                                         #\n",
        "#Date: March 07, 2025                                                          #\n",
        "#Course: CPSC 5440                                                             #\n",
        "#Topic: Assignment 2                                                           #\n",
        "#Description:                                                                  #\n",
        "#This program performs hyperparameter tuning on a neural network using the     #\n",
        "#Talos library with the CIFAR-100 dataset. It loads the dataset from           #\n",
        "#Google Drive, defines a search space for hyperparameters, and trains          #\n",
        "#models using different configurations. The best-performing model is           #\n",
        "#selected based on validation accuracy, and results are visualized using       #\n",
        "#a line plot. Finally, the script saves the results and plots to Google Drive  #\n",
        "#for further analysis.                                                         #\n",
        "################################################################################\n",
        "\n",
        "# Including Python libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import talos as ta\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# Connecting to Google drive\n",
        "drive.mount(\"/content/drive\")\n",
        "# Load CIFAR-100 dataset\n",
        "\n",
        "def load_data():\n",
        "  # Open train DB\n",
        "    with open('/content/drive/My Drive/train', 'rb') as file:\n",
        "        train_dict = pickle.load(file, encoding='bytes')\n",
        "  # Open test DB\n",
        "    with open('/content/drive/My Drive/test', 'rb') as file:\n",
        "        test_dict = pickle.load(file, encoding='bytes')\n",
        "\n",
        "    X_train = train_dict[b'data']\n",
        "    y_train = train_dict[b'coarse_labels']\n",
        "    X_test = test_dict[b'data']\n",
        "    y_test = test_dict[b'coarse_labels']\n",
        "\n",
        "    enc = OneHotEncoder(sparse_output=False, categories='auto')\n",
        "    y_train = enc.fit_transform(np.array(y_train).reshape(-1, 1))\n",
        "    y_test = enc.transform(np.array(y_test).reshape(-1, 1))\n",
        "\n",
        "    X_train = torch.tensor(X_train / 255.0, dtype=torch.float32).reshape(-1, 3072)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "    X_test = torch.tensor(X_test / 255.0, dtype=torch.float32).reshape(-1, 3072)\n",
        "    y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "# Define the neural network\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size=3072, hidden_size=240, num_classes=100):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, num_classes)\n",
        "        )\n",
        "        self.history = {'epoch': [], 'accuracy': []}  # Store history in the model\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten the input\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "# Train function\n",
        "def train_model(X_train, y_train, X_test, y_test, params):\n",
        "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "    model = SimpleNN(input_size=3072, hidden_size=params['units'], num_classes=100)\n",
        "    criterion = nn.CrossEntropyLoss() if params['loss'] == 'categorical_crossentropy' else nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters()) if params['optimizer'] == 'adam' else optim.Adagrad(model.parameters())\n",
        "\n",
        "    train_loader = data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=params['batch_size'], shuffle=True)\n",
        "    test_loader = data.DataLoader(torch.utils.data.TensorDataset(X_test, y_test), batch_size=params['batch_size'], shuffle=False)\n",
        "\n",
        "    #history = {'epoch': [], 'accuracy': []}\n",
        "    best_acc = 0.0\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(20):\n",
        "        model.train()\n",
        "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/20\", leave=True)  # Fixed tqdm\n",
        "\n",
        "        for batch_X, batch_y in loop:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "\n",
        "            if isinstance(criterion, nn.CrossEntropyLoss):\n",
        "                batch_y = torch.argmax(batch_y, dim=1).long()\n",
        "\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loop.set_postfix(loss=loss.item())  # Update progress bar\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in test_loader:\n",
        "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "                outputs = model(batch_X)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                _, labels = torch.max(batch_y, 1)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        accuracy = correct / total\n",
        "        print(f\"Epoch {epoch+1}/20: Accuracy = {accuracy * 100:.2f}%\")\n",
        "\n",
        "        model.history['epoch'].append(epoch)  # Fixed indentation\n",
        "        model.history['accuracy'].append(accuracy)\n",
        "\n",
        "        if accuracy > best_acc:\n",
        "            best_acc = accuracy\n",
        "\n",
        "    print(f\"Best Accuracy: {best_acc * 100:.2f}%\")\n",
        "    df = pd.DataFrame(model.history)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.lineplot(data=df, x='epoch', y='accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Test Accuracy')\n",
        "    plt.title('CIFAR-100 Training Results')\n",
        "    plt.grid()\n",
        "    plt.savefig(\"/content/drive/My Drive/cifar100_training_plot.png\", dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    return model, model.history\n",
        "\n",
        "# Define Talos parameter dictionary\n",
        "p = {\n",
        "    'units': [120, 240],\n",
        "    'hidden_activations': ['relu', 'sigmoid'],\n",
        "    'loss': ['categorical_crossentropy'],\n",
        "    'optimizer': ['adam', 'adagrad'],\n",
        "    'batch_size': [128, 256]\n",
        "}\n",
        "\n",
        "# Load data\n",
        "X_train, y_train, X_test, y_test = load_data()\n",
        "\n",
        "# Run Talos scan\n",
        "scan_object = ta.Scan(x=X_train.numpy(),\n",
        "        y=y_train.numpy(),\n",
        "        model=train_model,\n",
        "        params=p,\n",
        "        experiment_name='cifar100_talos')\n",
        "best_index = scan_object.data['val_accuracy'].idxmax()  # Find best accuracy index\n",
        "best_params = scan_object.data.iloc[best_index]  # Retrieve best hyperparameters\n",
        "\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(best_params)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPXkcXzzqk7Z7TqwpH8swRW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}