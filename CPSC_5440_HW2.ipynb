{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPw1uvXySWY3HKRCQfX0pMu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahzaidy/Programs/blob/main/CPSC_5440_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KBCLx8uiVt0"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "#Author: Arif H. Zaidy                                                         #\n",
        "#Date: March 05, 2025                                                          #\n",
        "#Course: CPSC 5440                                                             #\n",
        "#Topic: Assignment 2                                                           #\n",
        "#Description:                                                                  #\n",
        "#This program performs hyperparameter tuning on a neural network using the     #\n",
        "#Talos library with the CIFAR-100 dataset. It loads the dataset from          #\n",
        "#Google Drive, defines a search space for hyperparameters, and trains          #\n",
        "#models using different configurations. The best-performing model is          #\n",
        "#selected based on validation accuracy, and results are visualized using       #\n",
        "#a line plot. Finally, the script saves the results and plots to Google        #\n",
        "#Drive for further analysis.                                                   #\n",
        "################################################################################\n",
        "\n",
        "import talos\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import time\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access stored files\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Load CIFAR-100 dataset from Google Drive\n",
        "with open('/content/drive/My Drive/train', 'rb') as file:\n",
        "    train_dict = pickle.load(file, encoding='bytes')\n",
        "\n",
        "with open('/content/drive/My Drive/test', 'rb') as file:\n",
        "    test_dict = pickle.load(file, encoding='bytes')\n",
        "\n",
        "# Extract training and testing data\n",
        "X_train = train_dict[b'data']\n",
        "y_train = train_dict[b'coarse_labels']\n",
        "X_test = test_dict[b'data']\n",
        "y_test = test_dict[b'coarse_labels']\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 100)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 100)\n",
        "\n",
        "# Define hyperparameter search space for Talos\n",
        "p = {\n",
        "    'units': [120, 240],  # Number of neurons in hidden layers\n",
        "    'hidden_activations': ['relu', 'sigmoid'],  # Activation functions for hidden layers\n",
        "    'activation': ['softmax', 'sigmoid'],  # Activation function for output layer\n",
        "    'loss': ['mse', 'categorical_crossentropy'],  # Loss functions to test\n",
        "    'optimizer': ['adam', 'adagrad'],  # Optimizers to experiment with\n",
        "    'batch_size': [1000, 2000]  # Different batch sizes for training\n",
        "}\n",
        "\n",
        "# Define the model function for Talos hyperparameter tuning\n",
        "def my_model(X_train, y_train, X_val, y_val, params):\n",
        "    model = Sequential()\n",
        "    # Input layer with specified number of units and activation function\n",
        "    model.add(Dense(units=params['units'], activation=params['hidden_activations'], input_dim=X_train.shape[1]))\n",
        "\n",
        "    # Adding 4 hidden layers with same units and activation function\n",
        "    for _ in range(4):\n",
        "        model.add(Dense(units=params['units'], activation=params['hidden_activations']))\n",
        "\n",
        "    # Output layer with 100 units (for 100 classes) and chosen activation function\n",
        "    model.add(Dense(units=100, activation=params['activation']))\n",
        "\n",
        "    # Compile the model with selected loss function and optimizer\n",
        "    model.compile(loss=params['loss'], optimizer=params['optimizer'], metrics=['accuracy'])\n",
        "\n",
        "    # Train the model using provided training and validation data\n",
        "    out = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
        "                    batch_size=params['batch_size'], epochs=200, verbose=0)\n",
        "\n",
        "    return out, model\n",
        "\n",
        "# Define the output file path for Talos results\n",
        "csv_filename = \"/drive/My Drive/talos_output\"\n",
        "\n",
        "# Start hyperparameter tuning using Talos\n",
        "start_time = time.time()\n",
        "t_object = talos.Scan(x=X_train, y=y_train, params=p, model=my_model, x_val=X_test, y_val=y_test, round_limit=10, experiment_name=csv_filename)\n",
        "\n",
        "df = t_object.data  # Retrieve results as a dataframe\n",
        "\n",
        "# Find the best performing hyperparameter set based on training accuracy\n",
        "best_accuracy = df['accuracy'].max() * 100  # Convert to percentage\n",
        "best_accuracy_rounded = round(best_accuracy)\n",
        "\n",
        "# Calculate point deduction based on accuracy threshold\n",
        "deduction = max(0, 20 - best_accuracy_rounded)\n",
        "\n",
        "# Find the best-performing model based on validation accuracy\n",
        "best_model_idx = df['val_accuracy'].idxmax()  # Get the row index with highest validation accuracy\n",
        "best_params = df.iloc[best_model_idx]  # Extract best hyperparameters\n",
        "\n",
        "# Print the best-performing hyperparameter set\n",
        "print(\"\\nBest Performing Hyperparameters:\")\n",
        "print(best_params)\n",
        "\n",
        "# Display results\n",
        "print(f\"\\nBest Training Accuracy: {best_accuracy:.2f}% (Rounded: {best_accuracy_rounded}%)\")\n",
        "print(f\"Point Deduction: {deduction} points\")\n",
        "print(f\"Total Execution Time: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Sort results by validation accuracy\n",
        "df_sorted = df.sort_values(by=\"val_accuracy\", ascending=False)\n",
        "\n",
        "# Plot validation accuracy for each trial\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.lineplot(x=range(len(df_sorted)), y=df_sorted[\"val_accuracy\"], marker=\"o\")\n",
        "\n",
        "plt.xlabel(\"Trial Number\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.title(\"Hyperparameter Tuning Results\")\n",
        "plt.grid()\n",
        "\n",
        "# Save the figure to Google Drive\n",
        "plt.savefig(\"/content/drive/My Drive/hyperparameter_tuning_plot.png\", dpi=300)\n",
        "plt.show()"
      ]
    }
  ]
}