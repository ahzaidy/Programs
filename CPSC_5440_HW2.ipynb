{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahzaidy/Programs/blob/main/CPSC_5440_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7iuZVNMWq96F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "3e2bb8c3-8b62-4181-b810-1bca30711596"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 391/391 [00:02<00:00, 140.57it/s, loss=2.49]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5: Accuracy = 21.42%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 391/391 [00:03<00:00, 123.07it/s, loss=2.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5: Accuracy = 26.18%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 391/391 [00:03<00:00, 116.85it/s, loss=2.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5: Accuracy = 27.45%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 391/391 [00:03<00:00, 118.97it/s, loss=2.06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5: Accuracy = 28.46%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 391/391 [00:03<00:00, 111.82it/s, loss=2.27]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5: Accuracy = 30.48%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataFrame' object has no attribute 'append'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-982d0aad8c11>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mdf_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'training_iteration'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hidden_activation'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hidden_activations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6301\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
          ]
        }
      ],
      "source": [
        "################################################################################\n",
        "#Author: Arif H. Zaidy                                                         #\n",
        "#Date: March 07, 2025                                                          #\n",
        "#Course: CPSC 5440                                                             #\n",
        "#Topic: Assignment 2                                                           #\n",
        "#Description:                                                                  #\n",
        "#This program performs hyperparameter tuning on a neural network using         #\n",
        "#the CIFAR-100 dataset. It loads the dataset from Google Drive, defines        #\n",
        "#a search space for hyperparameters, and trains models using different         #\n",
        "#configurations. The best-performing model is selected based on validation     #\n",
        "#accuracy, and results are visualized using a line plot. Finally, the script   #\n",
        "#saves the results and plots to Google Drive for further analysis.             #\n",
        "################################################################################\n",
        "\n",
        "# Including Python libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from itertools import product\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# Connecting to Google Drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "def load_data():\n",
        "    with open('/content/drive/My Drive/train', 'rb') as file:\n",
        "        train_dict = pickle.load(file, encoding='bytes')\n",
        "    with open('/content/drive/My Drive/test', 'rb') as file:\n",
        "        test_dict = pickle.load(file, encoding='bytes')\n",
        "\n",
        "    X_train = train_dict[b'data']\n",
        "    y_train = train_dict[b'coarse_labels']\n",
        "    X_test = test_dict[b'data']\n",
        "    y_test = test_dict[b'coarse_labels']\n",
        "\n",
        "    enc = OneHotEncoder(sparse_output=False, categories='auto')\n",
        "    y_train = enc.fit_transform(np.array(y_train).reshape(-1, 1))\n",
        "    y_test = enc.transform(np.array(y_test).reshape(-1, 1))\n",
        "\n",
        "    X_train = torch.tensor(X_train / 255.0, dtype=torch.float32).reshape(-1, 3072)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "    X_test = torch.tensor(X_test / 255.0, dtype=torch.float32).reshape(-1, 3072)\n",
        "    y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "# Define the neural network\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size=3072, hidden_size=240, num_classes=100, activation_fn=nn.ReLU):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            activation_fn(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            activation_fn(),\n",
        "            nn.Linear(hidden_size, num_classes)\n",
        "        )\n",
        "        self.history = {'epoch': [], 'accuracy': []}  # Store history in the model\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten the input\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "# Train function\n",
        "def train_model(X_train, y_train, X_test, y_test, params):\n",
        "    model = SimpleNN(input_size=3072, hidden_size=params['units'], num_classes=100,\n",
        "                      activation_fn=nn.ReLU if params['hidden_activations'] == 'relu' else nn.Sigmoid)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss() if params['loss'] == 'categorical_crossentropy' else nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters()) if params['optimizer'] == 'adam' else optim.Adagrad(model.parameters())\n",
        "\n",
        "    train_loader = data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=params['batch_size'], shuffle=True)\n",
        "    test_loader = data.DataLoader(torch.utils.data.TensorDataset(X_test, y_test), batch_size=params['batch_size'], shuffle=False)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(5):\n",
        "        model.train()\n",
        "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/5\", leave=True)\n",
        "\n",
        "        for batch_X, batch_y in loop:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "\n",
        "            if isinstance(criterion, nn.CrossEntropyLoss):\n",
        "                batch_y = torch.argmax(batch_y, dim=1).long()\n",
        "\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in test_loader:\n",
        "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "                outputs = model(batch_X)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                _, labels = torch.max(batch_y, 1)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        accuracy = correct / total\n",
        "        print(f\"Epoch {epoch+1}/5: Accuracy = {accuracy * 100:.2f}%\")\n",
        "\n",
        "        model.history['epoch'].append(epoch)\n",
        "        model.history['accuracy'].append(accuracy)\n",
        "\n",
        "        if accuracy > best_acc:\n",
        "            best_acc = accuracy\n",
        "\n",
        "    return model, model.history\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'units': [120, 240],\n",
        "    'hidden_activations': ['relu', 'sigmoid'],\n",
        "    'loss': ['categorical_crossentropy'],\n",
        "    'optimizer': ['adam', 'adagrad'],\n",
        "    'batch_size': [128, 256]\n",
        "}\n",
        "\n",
        "param_combinations = list(product(*param_grid.values()))\n",
        "\n",
        "# Load data\n",
        "X_train, y_train, X_test, y_test = load_data()\n",
        "\n",
        "# Generate results DataFrame\n",
        "df_results = []\n",
        "iteration = 0\n",
        "for param_values in param_combinations:\n",
        "    params = dict(zip(param_grid.keys(), param_values))\n",
        "    _, history = train_model(X_train, y_train, X_test, y_test, params)\n",
        "    for epoch, acc in zip(history['epoch'], history['accuracy']):\n",
        "        df_results.append({'training_iteration': iteration, 'accuracy': acc, 'hidden_activation': params['hidden_activations']})\n",
        "    iteration += 1\n",
        "\n",
        "df_results = pd.DataFrame(df_results)\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.lineplot(data=df_results, x='training_iteration', y='accuracy', hue='hidden_activation')\n",
        "plt.xlabel('Training Iteration')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.title('Hyperparameter Tuning Results')\n",
        "plt.grid()\n",
        "plt.savefig(\"/content/drive/My Drive/hyperparameter_tuning_plot.png\", dpi=300)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMzKXRUWWZWzQdLfmAyPgsj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}