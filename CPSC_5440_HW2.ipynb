{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahzaidy/Programs/blob/main/CPSC_5440_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iuZVNMWq96F"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "#Author: Arif H. Zaidy                                                         #\n",
        "#Date: March 07, 2025                                                          #\n",
        "#Course: CPSC 5440                                                             #\n",
        "#Topic: Assignment 2                                                           #\n",
        "#Description:                                                                  #\n",
        "#This program performs hyperparameter tuning on a neural network using         #\n",
        "#the CIFAR-100 dataset. It loads the dataset from Google Drive, defines        #\n",
        "#a search space for hyperparameters, and trains models using different         #\n",
        "#configurations. The best-performing model is selected based on validation     #\n",
        "#accuracy, and results are visualized using a line plot. Finally, the script   #\n",
        "#saves the results and plots to Google Drive for further analysis.             #\n",
        "################################################################################\n",
        "\n",
        "# Including Python libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from itertools import product\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# Connecting to Google Drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "def load_data():\n",
        "    with open('/content/drive/My Drive/train', 'rb') as file:\n",
        "        train_dict = pickle.load(file, encoding='bytes')\n",
        "    with open('/content/drive/My Drive/test', 'rb') as file:\n",
        "        test_dict = pickle.load(file, encoding='bytes')\n",
        "\n",
        "    X_train = train_dict[b'data']\n",
        "    y_train = train_dict[b'coarse_labels']\n",
        "    X_test = test_dict[b'data']\n",
        "    y_test = test_dict[b'coarse_labels']\n",
        "\n",
        "    enc = OneHotEncoder(sparse_output=False, categories='auto')\n",
        "    y_train = enc.fit_transform(np.array(y_train).reshape(-1, 1))\n",
        "    y_test = enc.transform(np.array(y_test).reshape(-1, 1))\n",
        "\n",
        "    X_train = torch.tensor(X_train / 255.0, dtype=torch.float32).reshape(-1, 3072)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "    X_test = torch.tensor(X_test / 255.0, dtype=torch.float32).reshape(-1, 3072)\n",
        "    y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "# Define the neural network\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size=3072, hidden_size=240, num_classes=100, activation_fn=nn.ReLU):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            activation_fn(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            activation_fn(),\n",
        "            nn.Linear(hidden_size, num_classes)\n",
        "        )\n",
        "        self.history = {'epoch': [], 'accuracy': []}  # Store history in the model\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten the input\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "# Train function\n",
        "def train_model(X_train, y_train, X_test, y_test, params):\n",
        "    model = SimpleNN(input_size=3072, hidden_size=params['units'], num_classes=100,\n",
        "                      activation_fn=nn.ReLU if params['hidden_activations'] == 'relu' else nn.Sigmoid)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss() if params['loss'] == 'categorical_crossentropy' else nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters()) if params['optimizer'] == 'adam' else optim.Adagrad(model.parameters())\n",
        "\n",
        "    train_loader = data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=params['batch_size'], shuffle=True)\n",
        "    test_loader = data.DataLoader(torch.utils.data.TensorDataset(X_test, y_test), batch_size=params['batch_size'], shuffle=False)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(5):\n",
        "        model.train()\n",
        "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/5\", leave=True)\n",
        "\n",
        "        for batch_X, batch_y in loop:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "\n",
        "            if isinstance(criterion, nn.CrossEntropyLoss):\n",
        "                batch_y = torch.argmax(batch_y, dim=1).long()\n",
        "\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in test_loader:\n",
        "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "                outputs = model(batch_X)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                _, labels = torch.max(batch_y, 1)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        accuracy = correct / total\n",
        "        print(f\"Epoch {epoch+1}/5: Accuracy = {accuracy * 100:.2f}%\")\n",
        "\n",
        "        model.history['epoch'].append(epoch)\n",
        "        model.history['accuracy'].append(accuracy)\n",
        "\n",
        "        if accuracy > best_acc:\n",
        "            best_acc = accuracy\n",
        "\n",
        "    return model, model.history\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'units': [120, 240],\n",
        "    'hidden_activations': ['relu', 'sigmoid'],\n",
        "    'loss': ['categorical_crossentropy'],\n",
        "    'optimizer': ['adam', 'adagrad'],\n",
        "    'batch_size': [128, 256]\n",
        "}\n",
        "\n",
        "# Load data\n",
        "X_train, y_train, X_test, y_test = load_data()\n",
        "\n",
        "# Generate all possible combinations\n",
        "param_combinations = list(product(*param_grid.values()))\n",
        "\n",
        "best_accuracy = 0.0\n",
        "best_params = None\n",
        "best_model = None\n",
        "\n",
        "# Iterate through each parameter combination\n",
        "for param_values in param_combinations:\n",
        "    params = dict(zip(param_grid.keys(), param_values))\n",
        "    print(f\"Training with parameters: {params}\")\n",
        "    model, history = train_model(X_train, y_train, X_test, y_test, params)\n",
        "\n",
        "    max_accuracy = max(history['accuracy'])\n",
        "\n",
        "    if max_accuracy > best_accuracy:\n",
        "        best_accuracy = max_accuracy\n",
        "        best_params = params\n",
        "        best_model = model\n",
        "\n",
        "print(\"\\nBest Hyperparameters:\")\n",
        "print(best_params)\n",
        "print(f\"Best Accuracy: {best_accuracy * 100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPo5GHTp+oGeoMfV1JwdCmJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}