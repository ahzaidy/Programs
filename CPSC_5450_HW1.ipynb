{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFBC6KfogiwRqptFLmLoL7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahzaidy/Programs/blob/main/CPSC_5450_HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load word files\n",
        "def load_words(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        return [line.strip() for line in file.readlines()]\n",
        "\n",
        "english_words = load_words('https://raw.githubusercontent.com/ahzaidy/Programs/refs/heads/main/english.txt')\n",
        "german_words = load_words('https://raw.githubusercontent.com/ahzaidy/Programs/refs/heads/main/german.txt')\n",
        "spanish_words = load_words('https://raw.githubusercontent.com/ahzaidy/Programs/refs/heads/main/spanish.txt')\n",
        "\n",
        "# Filter only 5-letter words\n",
        "def filter_five_letter_words(word_list):\n",
        "    return [word for word in word_list if len(word) == 5]\n",
        "\n",
        "english_words = filter_five_letter_words(english_words)\n",
        "german_words = filter_five_letter_words(german_words)\n",
        "spanish_words = filter_five_letter_words(spanish_words)\n",
        "\n",
        "# Assign labels: English = 0, German = 1, Spanish = 2\n",
        "english_labels = [0] * len(english_words)\n",
        "german_labels = [1] * len(german_words)\n",
        "spanish_labels = [2] * len(spanish_words)\n",
        "\n",
        "# Combine datasets\n",
        "all_words = english_words + german_words + spanish_words\n",
        "all_labels = english_labels + german_labels + spanish_labels\n",
        "\n",
        "# Convert words to numerical features using language-specific mappings\n",
        "def create_char_mapping(language):\n",
        "    if language == \"english\":\n",
        "        alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    elif language == \"german\":\n",
        "        alphabet = 'abcdefghijklmnopqrstuvwxyzäöüß'\n",
        "    elif language == \"spanish\":\n",
        "        alphabet = 'abcdefghijklmnopqrstuvwxyzáéíóúüñ'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported language\")\n",
        "\n",
        "    mapping = {}\n",
        "    for idx, char in enumerate(alphabet, start=1):\n",
        "        mapping[char] = idx\n",
        "    return mapping\n",
        "\n",
        "english_mapping = create_char_mapping(\"english\")\n",
        "german_mapping = create_char_mapping(\"german\")\n",
        "spanish_mapping = create_char_mapping(\"spanish\")\n",
        "\n",
        "def word_to_features(word, language):\n",
        "    if language == 0:\n",
        "        mapping = english_mapping\n",
        "    elif language == 1:\n",
        "        mapping = german_mapping\n",
        "    elif language == 2:\n",
        "        mapping = spanish_mapping\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported language label\")\n",
        "\n",
        "    return [mapping.get(char.lower(), 0) for char in word]\n",
        "\n",
        "features = []\n",
        "for word, label in zip(all_words, all_labels):\n",
        "    features.append(word_to_features(word, label))\n",
        "\n",
        "# Create a DataFrame for features and labels\n",
        "data = pd.DataFrame(features)\n",
        "data['label'] = all_labels\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "max_length = data.shape[1] - 1\n",
        "data = data.apply(lambda row: row.fillna(0), axis=1)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X = data.iloc[:, :-1]\n",
        "y = data['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and evaluate models\n",
        "models = {\n",
        "    \"KNN\": KNeighborsClassifier(n_neighbors=3),\n",
        "    \"SVM\": SVC(kernel='linear', probability=True),\n",
        "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results[model_name] = accuracy\n",
        "    print(f\"{model_name} Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Plot results\n",
        "plt.bar(results.keys(), results.values())\n",
        "plt.title('Model Accuracy Comparison')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Model')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "jV0BW535GNXV",
        "outputId": "06077a79-44a4-480c-96aa-2bf0328e89a0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'https://raw.githubusercontent.com/ahzaidy/Programs/refs/heads/main/english.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2119e84b30e0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0menglish_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://raw.githubusercontent.com/ahzaidy/Programs/refs/heads/main/english.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mgerman_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://raw.githubusercontent.com/ahzaidy/Programs/refs/heads/main/german.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mspanish_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://raw.githubusercontent.com/ahzaidy/Programs/refs/heads/main/spanish.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-2119e84b30e0>\u001b[0m in \u001b[0;36mload_words\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Load word files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'https://raw.githubusercontent.com/ahzaidy/Programs/refs/heads/main/english.txt'"
          ]
        }
      ]
    }
  ]
}